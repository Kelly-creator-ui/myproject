{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import weight_norm\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the max value of dim0 is:1.0, the min value is:0.0\n",
      "the max value of dim1 is:1.0, the min value is:0.0\n",
      "the max value of dim2 is:1.0, the min value is:0.0\n",
      "train_input.shape: torch.Size([39552, 192, 2]), train_output.shape: torch.Size([39552, 96, 1]), \n",
      "val_input.shape: torch.Size([0, 192, 2]), val_output.shape: torch.Size([0, 96, 1])\n"
     ]
    }
   ],
   "source": [
    "from OptimalSena7daysInput.Models_for_7days.Model import TemporalConvNet, Encoder, Decoder, Transformer\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "\n",
    "from DHW_create_dataset2 import get_DataLoader\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "## data.npy是只维持4个步长的，optSOCdata.npy是全发1的\n",
    "data = np.array(pd.read_csv('data/0803 extended data 2.csv')[[\"ls\", \"tariff\", \"p_flex\"]])[206*96*2:,]\n",
    "power = np.array(pd.read_csv('data/0728 200user.csv')[[\"p_norm\", \"tariff\"]])[96:, ]\n",
    "batch_sz = 192 * 1\n",
    "# load data\n",
    "train_loader, val_loader, test_input, test_output, test_LS = get_DataLoader(data, power, batch_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9888, 192, 2]) torch.Size([9888, 96, 1]) torch.Size([9888, 96, 2])\n"
     ]
    }
   ],
   "source": [
    "print(test_input.shape, test_output.shape, test_LS.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6180, 96, 1])\n"
     ]
    }
   ],
   "source": [
    "test_input = test_input[-30*206:, :, :]\n",
    "test_output = test_output[-30*206:, :, :]\n",
    "test_LS = test_LS[-30*206:, :, :]\n",
    "print(test_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 2 # p_norm tariff\n",
    "CNN_Channels = [64, 64, 64, 64]\n",
    "Kernel_SZ = 4\n",
    "OUTPUT_DIM = 1 # p_flex\n",
    "HID_DIM = 128\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 256\n",
    "DEC_PF_DIM = 256\n",
    "CNN_DROPOUT = 0.1\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "\n",
    "down_sample_cnn = TemporalConvNet(INPUT_DIM,\n",
    "                                  CNN_Channels,\n",
    "                                  device,\n",
    "                                  Kernel_SZ,\n",
    "                                  CNN_DROPOUT)\n",
    "\n",
    "enc = Encoder(CNN_Channels[-1],\n",
    "              HID_DIM,\n",
    "              ENC_LAYERS,\n",
    "              ENC_HEADS,\n",
    "              ENC_PF_DIM,\n",
    "              ENC_DROPOUT,\n",
    "              device)\n",
    "\n",
    "dec = Decoder(OUTPUT_DIM,\n",
    "              HID_DIM,\n",
    "              DEC_LAYERS,\n",
    "              DEC_HEADS,\n",
    "              DEC_PF_DIM,\n",
    "              DEC_DROPOUT,\n",
    "              device)\n",
    "\n",
    "SRC_PAD_IDX = 10\n",
    "TRG_PAD_IDX = 10\n",
    "\n",
    "model = Transformer(down_sample_cnn, enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load ('models/0803 Transformer_model e-03.pt', map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(src, LS, model, max_len=96):\n",
    "    model.eval()\n",
    "    LS = LS.to(device)\n",
    "    src_tensor = src.to(device)\n",
    "    src_tensor = model.downsample_cnn(src_tensor.permute(0,2,1)).permute(0,2,1)\n",
    "    src_tensor = src_tensor[:,-96:,:]\n",
    "    src_mask = model.make_src_mask(src_tensor)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        enc_src = model.encoder(src_tensor, src_mask)\n",
    "\n",
    "    trg_indexes = torch.zeros(src.shape[0], 1, 1).to(device)\n",
    "\n",
    "    for i in range(max_len):\n",
    "\n",
    "        trg_tensor = torch.cat((trg_indexes, LS[:, :i+1, :]), dim=2)\n",
    "\n",
    "        trg_mask = model.make_trg_mask(trg_tensor)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
    "\n",
    "        trg_indexes = torch.cat((trg_indexes, output[:, -1:, :]), dim=1)\n",
    "\n",
    "    return trg_indexes[:, 1:, :], attention\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_times = 10  #拆开数据后的计算次数，直接计算会显存不足\n",
    "batch = test_input.shape[0]/cur_times #注意一定要能被整除\n",
    "batch = int(batch)\n",
    "\n",
    "predicted_output = torch.Tensor([])\n",
    "for i in range(cur_times):\n",
    "\n",
    "    temp, _ = translate_sentence(torch.Tensor(test_input[batch * i:batch * (i+1)]),torch.Tensor(test_LS[batch * i:batch * (i+1)]),\\\n",
    "                              model)\n",
    "    predicted_output = torch.cat((predicted_output, temp.cpu().detach()), dim=0)\n",
    "\n",
    "predicted_output = predicted_output.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch-cpu]",
   "language": "python",
   "name": "conda-env-pytorch-cpu-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
